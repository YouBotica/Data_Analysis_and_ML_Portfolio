{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "492daabb",
   "metadata": {},
   "source": [
    "## Random Forests Overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3383856",
   "metadata": {},
   "source": [
    "### Random Forests Hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9b2a36",
   "metadata": {},
   "source": [
    "1. Number of estimators: number of decision trees to use in the Random Forest.\n",
    "\n",
    "2. Number of features: how many features to include in each subset.\n",
    "\n",
    "3. Bootstrap samples: Allow for bootstrap sampling of each training subset of features.\n",
    "\n",
    "4. Out-of-bag error: Calculate OOB error during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff870ae2",
   "metadata": {},
   "source": [
    "Random forests don't overfit, you can use as many as you want and they will run fast.\n",
    "- A good approach can be to plot Error vs number of trees. (Similar to the elbow method in KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eac7fe",
   "metadata": {},
   "source": [
    "#### After a certain number of trees, two things can occur:\n",
    "1. Different random selections don't reveal any more information.\n",
    "2. Trees become highly correlated.\n",
    "3. Different random selections are simply duplicating trees that have already been created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ae3f7",
   "metadata": {},
   "source": [
    "#### Number of features in the subset?\n",
    "\n",
    "- The original publication suggests subsets of $log_2(N+1)$ random features in subset given N number of total features.\n",
    "\n",
    "- Later suggestions establish that sqrt(N) (most common convention), and N/3 are suitable.\n",
    "\n",
    "#### Also:\n",
    "- An interesting difference between regression and classification is that the correlation increases quite slowly as the number of features used increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d05e223",
   "metadata": {},
   "source": [
    "#### What is bootstrapping?\n",
    "\n",
    "- A term used to describe \"random sampling with replacement\".\n",
    "- Recall for each split we are randomly selecting a subset of features.\n",
    "- This random subset of features helps create more diverse trees that are not correlated to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ef809",
   "metadata": {},
   "source": [
    "#### Out-of-bag error:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06033943",
   "metadata": {},
   "source": [
    "- When bootstrapping and creating trees, there is going to be some data that is not used for constructing a particular tree.\n",
    "So this data can be used to get performance test metrics on trees that did not use these rows!\n",
    "- Take into account that OOB doesn't really affect the training process.\n",
    "- It is separate from bootstrapping, OOB Score is an optional way of measuring performance, an alternative to using standard train/test split, since bootstrapping naturally results in unused data during training.\n",
    "- Note that OOB Score is also limited to not using all the trees in the random forest, it can only be calculated on trees that did not use the OOB data!\n",
    "- Due to not using the entire random forest, the default value of OOB score hyperparameter is set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982ef8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
